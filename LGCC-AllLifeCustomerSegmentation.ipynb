{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllLife Bank Customer Segmentation\n",
    "\n",
    "The Bank is running new marketing campaign for next year. The marketing team suggested to do a customer segmentation to focused those marketing to each customer groups in order to target on new customers as well as existing customers. The Operation teams wants to ensure that customer queries are resolved faster.\n",
    "\n",
    "### Objective\n",
    "Identify different segmentation in the existing customer, based on theri spending patterns as well as past interaction with the bank\n",
    "* Using clustering algoritms\n",
    "* Provide recommendations to the bank on how to better market to and service these customers.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "- How many different types (clusters/segments) of bank customers can be found from the data?\n",
    "- How do these different groups of customer differ from each other?\n",
    "- How to perform clustering using the components obtained from PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "Financial attributes of Bank Customers: credit limit, total number of credit cards, different contact channels\n",
    "\n",
    "**Data Dictionary**\n",
    "- Sl_No: Primary key of the records\n",
    "- Customer Key: Customer identification number\n",
    "- Average Credit Limit: Average credit limit of each customer for all credit cards\n",
    "- Total credit cards: Total number of credit cards possessed by the customer\n",
    "- Total visits bank: Total number of visits that customer made (yearly) personally to the bank\n",
    "- Total visits online: Total number of visits or online logins made by the customer (yearly)\n",
    "- Total calls made: Total number of calls made by the customer to the bank or its customer service department (yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 2;\n                var nbb_unformatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n# Import data manipulation libraries\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libraries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# to scale the data using z-score\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# to compute distances\\nfrom scipy.spatial.distance import cdist\\n\\n# to perform k-means clustering and compute silhouette scores\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\n# to visualize the elbow curve and silhouette scores\\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\\n\\n# to compute distances\\nfrom scipy.spatial.distance import pdist\\n\\n# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\\n\\n# to perform PCA\\nfrom sklearn.decomposition import PCA\";\n                var nbb_formatted_code = \"# this will help in making the Python code more structured automatically (good coding practice)\\n%load_ext nb_black\\n# Import data manipulation libraries\\nimport pandas as pd\\nimport numpy as np\\n\\n# Libraries to help with data visualization\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# to scale the data using z-score\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# to compute distances\\nfrom scipy.spatial.distance import cdist\\n\\n# to perform k-means clustering and compute silhouette scores\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import silhouette_score\\n\\n# to visualize the elbow curve and silhouette scores\\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\\n\\n# to compute distances\\nfrom scipy.spatial.distance import pdist\\n\\n# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\\nfrom sklearn.cluster import AgglomerativeClustering\\nfrom scipy.cluster.hierarchy import dendrogram, linkage, cophenet\\n\\n# to perform PCA\\nfrom sklearn.decomposition import PCA\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this will help in making the Python code more structured automatically (good coding practice)\n",
    "%load_ext nb_black\n",
    "# Import data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries to help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to scale the data using z-score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# to compute distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# to perform k-means clustering and compute silhouette scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# to visualize the elbow curve and silhouette scores\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "\n",
    "# to compute distances\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "\n",
    "# to perform PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n            setTimeout(function() {\n                var nbb_cell_id = 8;\n                var nbb_unformatted_code = \"## Read Data \\n## S1_No can be use as index column\\ndata = pd.read_excel(\\\"CreditCardCustomerData.xlsx\\\", index_col=\\\"Sl_No\\\")\";\n                var nbb_formatted_code = \"## Read Data\\n## S1_No can be use as index column\\ndata = pd.read_excel(\\\"CreditCardCustomerData.xlsx\\\", index_col=\\\"Sl_No\\\")\";\n                var nbb_cells = Jupyter.notebook.get_cells();\n                for (var i = 0; i < nbb_cells.length; ++i) {\n                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n                             nbb_cells[i].set_text(nbb_formatted_code);\n                        }\n                        break;\n                    }\n                }\n            }, 500);\n            ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Read Data \n",
    "## S1_No can be use as index column\n",
    "data = pd.read_excel(\"CreditCardCustomerData.xlsx\", index_col=\"Sl_No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the problem and perform an Exploratory Data Analysis\n",
    "Problem definition, questions to be answered - Data background and contents - Univariate analysis - Bivariate analysis - Insights based on EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing\n",
    "Prepare the data for analysis - Feature engineering - Missing value treatment - Outlier treatment - Duplicate observations check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "\n",
    "\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting numerical columns\n",
    "num_cols = df1.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "for item in num_cols:\n",
    "    histogram_boxplot(df1, item, bins=50, kde=True, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bivariate - Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "sns.heatmap(\n",
    "    df1[num_cols].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df[num_col], diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df1.iloc[:, 2:].columns.tolist():\n",
    "    df1[col] = df1.groupby([\"Region\"])[col].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# checking for missing values\n",
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data set before clustering\n",
    "scaler = StandardScaler()\n",
    "subset = df[num_col].copy()\n",
    "subset_scaled = scaler.fit_transform(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "subset_scaled_df = pd.DataFrame(\n",
    "    sc.fit_transform(df1.drop([\"country\", \"Region\"], axis=1)),\n",
    "    columns=df1.drop([\"country\", \"Region\"], axis=1).columns,\n",
    ")\n",
    "subset_scaled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying K-means Clustering\n",
    "Apply K-means Clustering - Plot the Elbow curve - Check Silhouette Scores - Figure out appropriate number of clusters - Cluster Profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = range(1, 9)\n",
    "meanDistortions = []\n",
    "\n",
    "for k in clusters:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(subset_scaled_df)\n",
    "    prediction = model.predict(subset_scaled_df)\n",
    "    distortion = (\n",
    "        sum(\n",
    "            np.min(cdist(subset_scaled_df, model.cluster_centers_, \"euclidean\"), axis=1)\n",
    "        )\n",
    "        / subset_scaled_df.shape[0]\n",
    "    )\n",
    "\n",
    "    meanDistortions.append(distortion)\n",
    "\n",
    "    print(\"Number of Clusters:\", k, \"\\tAverage Distortion:\", distortion)\n",
    "\n",
    "plt.plot(clusters, meanDistortions, \"bx-\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Average Distortion\")\n",
    "plt.title(\"Selecting k with the Elbow Method\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score = []\n",
    "cluster_list = list(range(2, 10))\n",
    "for n_clusters in cluster_list:\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict((subset_scaled_df))\n",
    "    # centers = clusterer.cluster_centers_\n",
    "    score = silhouette_score(subset_scaled_df, preds)\n",
    "    sil_score.append(score)\n",
    "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
    "\n",
    "plt.plot(cluster_list, sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(7, random_state=1))\n",
    "visualizer.fit(subset_scaled_df)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(6, random_state=1))\n",
    "visualizer.fit(subset_scaled_df)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(5, random_state=1))\n",
    "visualizer.fit(subset_scaled_df)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding optimal no. of clusters with silhouette coefficients\n",
    "visualizer = SilhouetteVisualizer(KMeans(4, random_state=1))\n",
    "visualizer.fit(subset_scaled_df)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "kmeans.fit(subset_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding kmeans cluster labels to the original dataframe\n",
    "df[\"K_means_segments\"] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile = df.groupby(\"K_means_segments\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile[\"count_in_each_segment\"] = (\n",
    "    df.groupby(\"K_means_segments\")[\"Fees\"].count().values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's display cluster profiles\n",
    "cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(16, 6))\n",
    "fig.suptitle(\"Boxplot of numerical variables for each cluster\")\n",
    "counter = 0\n",
    "for ii in range(5):\n",
    "    sns.boxplot(ax=axes[ii], y=df[num_col[counter]], x=df[\"K_means_segments\"])\n",
    "    counter = counter + 1\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Hierarchical Clustering\n",
    "Apply Hierarchical clustering with different linkage methods - Plot dendrograms for each linkage method - Check cophenetic correlation for each linkage method - Figure out appropriate number of clusters - Cluster Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of distance metrics\n",
    "distance_metrics = [\"euclidean\", \"chebyshev\", \"mahalanobis\", \"cityblock\"]\n",
    "\n",
    "# list of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"weighted\"]\n",
    "\n",
    "high_cophenet_corr = 0\n",
    "high_dm_lm = [0, 0]\n",
    "\n",
    "for dm in distance_metrics:\n",
    "    for lm in linkage_methods:\n",
    "        Z = linkage(subset_scaled_df, metric=dm, method=lm)\n",
    "        c, coph_dists = cophenet(Z, pdist(subset_scaled_df))\n",
    "        print(\n",
    "            \"Cophenetic correlation for {} distance and {} linkage is {}.\".format(\n",
    "                dm.capitalize(), lm, c\n",
    "            )\n",
    "        )\n",
    "        if high_cophenet_corr < c:\n",
    "            high_cophenet_corr = c\n",
    "            high_dm_lm[0] = dm\n",
    "            high_dm_lm[1] = lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the combination of distance metric and linkage method with the highest cophenetic correlation\n",
    "print(\n",
    "    \"Highest cophenetic correlation is {}, which is obtained with {} distance and {} linkage.\".format(\n",
    "        high_cophenet_corr, high_dm_lm[0].capitalize(), high_dm_lm[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of linkage methods\n",
    "linkage_methods = [\"single\", \"complete\", \"average\", \"centroid\", \"ward\", \"weighted\"]\n",
    "\n",
    "# lists to save results of cophenetic correlation calculation\n",
    "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
    "\n",
    "# to create a subplot image\n",
    "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 30))\n",
    "\n",
    "# We will enumerate through the list of linkage methods above\n",
    "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    Z = linkage(subset_scaled_df, metric=\"euclidean\", method=method)\n",
    "\n",
    "    dendrogram(Z, ax=axs[i])\n",
    "    axs[i].set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
    "\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(subset_scaled_df))\n",
    "    axs[i].annotate(\n",
    "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
    "        (0.80, 0.80),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of linkage methods\n",
    "linkage_methods = [\"centroid\"]\n",
    "\n",
    "# lists to save results of cophenetic correlation calculation\n",
    "compare_cols = [\"Linkage\", \"Cophenetic Coefficient\"]\n",
    "\n",
    "# to create a subplot image\n",
    "fig, axs = plt.subplots(len(linkage_methods), 1, figsize=(15, 15))\n",
    "\n",
    "# We will enumerate through the list of linkage methods above\n",
    "# For each linkage method, we will plot the dendrogram and calculate the cophenetic correlation\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    Z = linkage(subset_scaled_df, metric=\"euclidean\", method=method)\n",
    "\n",
    "    dendrogram(Z, ax=axs)\n",
    "    axs.set_title(f\"Dendrogram ({method.capitalize()} Linkage)\")\n",
    "\n",
    "    coph_corr, coph_dist = cophenet(Z, pdist(subset_scaled_df))\n",
    "    axs.annotate(\n",
    "        f\"Cophenetic\\nCorrelation\\n{coph_corr:0.2f}\",\n",
    "        (0.80, 0.80),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )\n",
    "dendrogram(Z, color_threshold=7.3)\n",
    "plt.axhline(y=7.3, c=\"red\", lw=1, linestyle=\"dashdot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HCmodel = AgglomerativeClustering(n_clusters=6, affinity=\"euclidean\", linkage=\"average\")\n",
    "HCmodel.fit(subset_scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_scaled_df[\"HC_Clusters\"] = HCmodel.labels_\n",
    "df1[\"HC_Clusters\"] = HCmodel.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile = df1.groupby(\"HC_Clusters\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_profile[\"count_in_each_segments\"] = (\n",
    "    df1.groupby(\"HC_Clusters\")[\"Surface area\"].count().values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the names of the countries in each cluster\n",
    "for cl in df1[\"HC_Clusters\"].unique():\n",
    "    print(\"In cluster {}, the following countries are present:\".format(cl))\n",
    "    print(df1[df1[\"HC_Clusters\"] == cl][\"country\"].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the names of the countries in each cluster\n",
    "for cl in df1[\"HC_Clusters\"].unique():\n",
    "    print(\n",
    "        \"The\",\n",
    "        df1[df1[\"HC_Clusters\"] == cl][\"country\"].nunique(),\n",
    "        \"countries in cluster\",\n",
    "        cl,\n",
    "        \"are:\",\n",
    "    )\n",
    "    print(df1[df1[\"HC_Clusters\"] == cl][\"country\"].unique())\n",
    "    print(\"-\" * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets display cluster profile\n",
    "cluster_profile.style.highlight_max(color=\"lightgreen\", axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means vs Hierarchical Clustering\n",
    "Compare clusters obtained from K-means and Hierarchical clustering techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 3, figsize=(15, 25))\n",
    "fig.suptitle(\"Boxplot of scaled numerical variables for each cluster\", fontsize=20)\n",
    "counter = 0\n",
    "for ii in range(4):\n",
    "    sns.boxplot(\n",
    "        ax=axes[ii][0],\n",
    "        y=subset_scaled_df[num_cols[counter]],\n",
    "        x=subset_scaled_df[\"HC_Clusters\"],\n",
    "    )\n",
    "    counter = counter + 1\n",
    "    sns.boxplot(\n",
    "        ax=axes[ii][1],\n",
    "        y=subset_scaled_df[num_cols[counter]],\n",
    "        x=subset_scaled_df[\"HC_Clusters\"],\n",
    "    )\n",
    "    counter = counter + 1\n",
    "    sns.boxplot(\n",
    "        ax=axes[ii][2],\n",
    "        y=subset_scaled_df[num_cols[counter]],\n",
    "        x=subset_scaled_df[\"HC_Clusters\"],\n",
    "    )\n",
    "    counter = counter + 1\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actionable Insights & Recommendations\n",
    "Conclude with the key takeaways for the business - What would be your recommendations to the business?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0a3e8d87c5cec14335f3b1a538ddb7a51ac547a3f57637c25f2965f3f1cbaa4c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
